<!DOCTYPE HTML>

<!-- 
Strata by HTML5 UP
html5up.net | @n33co
Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Write a Simple Crawler. &middot; Fred&#39;s blog</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<meta name="author" content="Fred Wong">
		<meta name="description" content="Fred&#39;s blog demo">
		<meta http-equiv="content-language" content="en-us" />

		
		<meta name="og:site_name" content="Fred&#39;s blog">
		<meta name="og:title" content="Write a Simple Crawler.">
		<meta name="og:url" content="https://haroldwo.github.io/abablog/post/crawler/">
		
		<meta name="og:image" content="https://haroldwo.github.io/abablog/images/avatar.PNG">
		
		

		<meta name="generator" content="Hugo 0.48" />

		<!--[if lte IE 8]><script src='https://haroldwo.github.io/abablog/js/ie/html5shiv.js'></script><![endif]-->
		<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
		<link rel="stylesheet" href="https://haroldwo.github.io/abablog/css/main.css" />
		<!--[if lte IE 8]><link rel="stylesheet" href="https://haroldwo.github.io/abablog//css/ie8.css"><![endif]-->

		
		
	</head>

	<body id="top">
		<!-- Header -->
<header id="header">
	
	<a href="https://haroldwo.github.io/abablog/" class="image avatar"><img src="https://haroldwo.github.io/abablog/images/avatar.PNG" alt="" /></a>
	
	
		<h1><strong>Hi! I&rsquo;m Fred Wong,</strong><br> an Open Source technophile.<br> This is my site. Enjoy!</h1>
	

	
		<nav id="sidebar">
			<ul>
			
				<li><a href="https://haroldwo.github.io/abablog/">Home</a></li>
			
				<li><a href="https://haroldwo.github.io/abablog/post/">Blog</a></li>
			
			</ul>
		</nav>
	
</header>


		<!-- Main -->
		<div id="main">
			
	<span>
		<h1>Write a Simple Crawler.</h1>

		<i class="fa fa-calendar"></i>&nbsp;&nbsp;
<time datetime="2018-09-20 12:00:00 &#43;0800 &#43;0800">2018-09-20</time>&nbsp;&nbsp;

4 min read&nbsp;&nbsp;


    
    
        <i class="fa fa-folder"></i>&nbsp;&nbsp;
        
        <a class="article-category-link" href="https://haroldwo.github.io/abablog/categories/dev">Dev</a>
        
        
    



   
   
       &nbsp;&nbsp;<i class="fa fa-tags"></i>&nbsp;&nbsp;
       
       <a class="article-category-link" href="https://haroldwo.github.io/abablog/tags/crawler">crawler</a>
       &middot;
       
       <a class="article-category-link" href="https://haroldwo.github.io/abablog/tags/goquery">goquery</a>
       
       
   


	</span>

	<p>
	    

<h2 id="1-basic-process">1. Basic process.</h2>

<h4 id="http-request-web-http-response-parse-data-handle-data">HTTP request -&gt; Web -&gt; HTTP response -&gt; Parse data -&gt; Handle data</h4>

<p>This is the basic process of a crawler program. I will not talk about concurrency or distributed framework here. This article aim to give a simple introduction of crawler.</p>

<p>A crawler actually is a script which execute simulation of our action very fast when we browse a web page and it follows some rules we designed for catching our data.</p>

<h2 id="2-describe-a-crawler">2. Describe a crawler.</h2>

<p>Let&rsquo;s describe the crawler as a object. What attribute can a crawler own? These attributes will be used as parameter in our program. Let me see. Well, First of all, I need a target URL for visit. Then, I need set my HTTP head when I send request to the website. Also, there could be more than one link when we search something like pictures so I need an array to store these links. Additionally, don&rsquo;t forget that the crawler will be blocked by a web server if it sends requests too frequently. We can set a optional interval mode for it. Ok, Let&rsquo;s code. Please follow this mind to create a stronger crawler if you want.</p>

<pre><code>type Crawler struct {
	targetUrl   string
	headHost    string
	headReferer string
	dataDir     string
	Contents    []string
	Mode
}

type Mode struct {
	name       string
	interval   *time.Ticker
	startPoint time.Time
	end        time.Timer
	duration   time.Duration
}
</code></pre>

<p>In most of cases, we only need to change host and referer of HTTP head. So I just involved these two.</p>

<h2 id="3-http-request-web">3. HTTP request -&gt; Web</h2>

<p>I used lib &ldquo;github.com/PuerkitoBio/goquery&rdquo; for convenience. You can use regexp instead of it for a more original way. Here, I wrote two methods.</p>

<p>First one, I just catch data from a page.</p>

<pre><code>func (c Crawler) getDoc() (*goquery.Document, error) {
	res, err := http.Get(c.targetUrl)
	if err != nil {
		log.Fatal(err)
		return nil, err
	}
	defer res.Body.Close()
	if res.StatusCode != 200 {
		log.Fatalf(&quot;status code error: %d %s&quot;, res.StatusCode, res.Status)
		return nil, errors.New(res.Status)
	}
	doc, err := goquery.NewDocumentFromReader(res.Body)
	if err != nil {
		log.Fatal(err)
		return nil, err
	}
	return doc, nil
}
</code></pre>

<p>Second one, I send different requests for further links.</p>

<pre><code>func (c Crawler) reqDoc() (*goquery.Document, error) {
	req, err := http.NewRequest(&quot;GET&quot;, c.targetUrl, nil)
	if err != nil {
		log.Fatal(err)
		return nil, err
	}
	header := map[string]string{
		&quot;Host&quot;:                      c.headHost,
		&quot;Connection&quot;:                &quot;keep-alive&quot;,
		&quot;Cache-Control&quot;:             &quot;max-age=0&quot;,
		&quot;Upgrade-Insecure-Requests&quot;: &quot;1&quot;,
		&quot;User-Agent&quot;:                &quot;Mozilla/5.0 (X11; Ubuntu; Linuâ€¦) Gecko/20100101 Firefox/62.0&quot;,
		&quot;Accept&quot;:                    &quot;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&quot;,
		&quot;Referer&quot;:                   c.headReferer,
	}
	for key, value := range header {
		req.Header.Add(key, value)
	}
	client := &amp;http.Client{}
	resp, err := client.Do(req)
	if err != nil {
		log.Fatal(err)
		return nil, err
	}
	defer resp.Body.Close()
	doc, err := goquery.NewDocumentFromReader(resp.Body)
	if err != nil {
		log.Fatal(err)
		return nil, err
	}
	return doc, nil
}
</code></pre>

<h2 id="4-http-response-parse-data">4. HTTP response -&gt; Parse data</h2>

<p>First, This is a convenient function you may use for remove duplicate data from a array such as urls. Just FYI.</p>

<pre><code>func Duplicate(dirty interface{}) (clean []interface{}) {
	va := reflect.ValueOf(dirty)
	for i := 0; i &lt; va.Len(); i++ {
		if i &gt; 0 &amp;&amp; reflect.DeepEqual(va.Index(i-1).Interface(), va.Index(i).Interface()) {
			continue
		}
		clean = append(clean, va.Index(i).Interface())
	}
	return clean
}
</code></pre>

<p>Then, Let&rsquo;s write a method that filter some info from the response.</p>

<pre><code>func (c *Crawler) getContent() error {
	doc, err := c.reqDoc()
	if err != nil {
		log.Fatal(err)
		return err
	}
	doc.Find(&quot;.content&quot;).Each(func(i int, s *goquery.Selection) {
		content := s.Find(&quot;a&quot;).Text()
		c.Contents = append(c.Contents, content)
	})
	return nil
}
</code></pre>

<p>This is another example for filtering picture info.</p>

<pre><code>func (c *Crawler) getPic() error {
	doc, err := c.reqDoc()
	if err != nil {
		log.Fatal(err)
		return err
	}
	doc.Find(&quot;img&quot;).Each(func(i int, s *goquery.Selection) {
		link, ok := s.Attr(&quot;src&quot;)
		if ok {
			c.Contents = append(c.Contents, link)
		} else {
			fmt.Println(&quot;Address not found.&quot;)
		}
	})
	return nil
}
</code></pre>

<h2 id="5-parse-data-handle-data">5. Parse data -&gt; Handle data</h2>

<p>Now, it&rsquo;s the turn we dealing with data.</p>

<p>In this example, I create a file and then write the data I got from the website.</p>

<pre><code>func (c Crawler) handleContent() error {
	fileName := filepath.Base(c.targetUrl)
	fullPath := filepath.Join(c.dataDir, fileName)
	file, err := os.Create(fullPath)
	if err != nil {
		log.Panic(&quot;Can not create file.&quot;)
		return err
	}
	for _, content := range Duplicate(c.contents) {
		file.WriteString(content.(string))
	}
	return nil
}
</code></pre>

<p>In this one, I download the pictures which I got info of from the website.</p>

<pre><code>func (c Crawler) handlePic() {
	for _, link := range c.contents {
		resp, err := http.Get(link)
		if err != nil {
			log.Fatal(err)
		}
		defer resp.Body.Close()
		if resp.StatusCode != http.StatusOK {
			log.Fatalf(&quot;status code error: %d %s&quot;, resp.StatusCode, resp.Status)
		}
		FileName := filepath.Base(link)
		FullName := filepath.Join(c.dataDir, FileName)
		file, err := os.Create(FullName)
		if err != nil {
			log.Panic(&quot;Can not create file.&quot;)
		}
		io.Copy(file, resp.Body)
	}
}
</code></pre>

<h2 id="6-continuation">6. Continuation.</h2>

<p>Now, we can write down our main function. Please try and remember to use timer to limit request. Crawler is very interesting and always used as important part in our application such as Big Data and AI. If you are a gopher, Let me introduce some projects for you.</p>

<ol>
<li><a href="https://github.com/PuerkitoBio/goquery">goquery</a> &ndash; a helpful lib for crawler.</li>
<li><a href="https://github.com/PuerkitoBio/gocrawl">gocrawl</a> &ndash; a light concurrent web crawler.</li>
<li><a href="https://github.com/henrylee2cn/pholcus">pholcus</a> &ndash; a heavy concurrent web crawler with complete distributed architecture.</li>
<li><a href="http://go-colly.org/docs/introduction/start/">colly</a> &ndash; the most popular crawler framework in Go.</li>
</ol>

<p>Thank you for your reading.</p>

	</p>

	<div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "haroldwo" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

		</div>

		<!-- Footer -->
<footer id="footer">
	<ul class="icons">
		
		
		
		
		<li><a href="//github.com/haroldwo" target="_blank" class="icon fa-github"><span class="label">GitHub</span></a></li>
		
		
		
		<li><a href="//linkedin.com/in/fredwo" target="_blank" class="icon fa-linkedin-square"><span class="label">Linkedin</span></a></li>
		
		
		
		
		<li><a href="https://haroldwo.github.io/abablog/#contact-form" class="icon fa-envelope-o"><span class="label">Email</span></a></li>
		
		
		<li><a href="https://haroldwo.github.io/abablog/index.xml" class="icon fa-rss" type="application/rss+xml"><span class="label">RSS</span></a></li>
		
	</ul>

	<ul class="copyright">
		
		<li>&copy; John Doe</li>
		
		<li>Design: <a href="//html5up.net">HTML5 UP</a></li>
		
		<li>Images: <a href="//unsplash.com/">Unsplash</a></li>
		
	</ul>
</footer>

<!-- Scripts -->
<script src="https://haroldwo.github.io/abablog/js/jquery.min.js"></script>
<script src="https://haroldwo.github.io/abablog/js/jquery.poptrox.min.js"></script>
<script src="https://haroldwo.github.io/abablog/js/skel.min.js"></script>
<script src="https://haroldwo.github.io/abablog/js/util.js"></script>

<script src="https://haroldwo.github.io/abablog/js/main.js"></script>


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-125977378-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>




	</body>
</html>
